{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean EDA setup for entities_dataset_v2 (safe version)\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"/home/sherin/Image_Captioning/entities_dataset_v2\").resolve()\n",
    "OUTPUT_DIR = Path(\"/home/sherin/Image_Captioning\").resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Exists:\", DATA_DIR.exists())\n",
    "\n",
    "\n",
    "def bytes_to_mb(num_bytes: int) -> float:\n",
    "\treturn round(num_bytes / (1024 * 1024), 3)\n",
    "\n",
    "\n",
    "def count_lines_fast(paths: List[Path]) -> Dict[str, int]:\n",
    "\tresult: Dict[str, int] = {}\n",
    "\tif not paths:\n",
    "\t\treturn result\n",
    "\ttry:\n",
    "\t\tcmd = [\"wc\", \"-l\", *[str(p) for p in paths]]\n",
    "\t\tproc = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "\t\tfor line in proc.stdout.strip().splitlines():\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif not line or line.endswith(\" total\"):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tparts = line.split()\n",
    "\t\t\tif len(parts) >= 2:\n",
    "\t\t\t\tline_count = int(parts[0])\n",
    "\t\t\t\tfile_path = parts[-1]\n",
    "\t\t\t\tresult[Path(file_path).name] = line_count\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"[warn] wc -l failed, falling back to Python:\", e)\n",
    "\t\tfor p in paths:\n",
    "\t\t\ttry:\n",
    "\t\t\t\twith open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "\t\t\t\t\tresult[p.name] = sum(1 for _ in f)\n",
    "\t\t\texcept Exception as e2:\n",
    "\t\t\t\tprint(f\"[warn] count lines failed for {p.name}: {e2}\")\n",
    "\t\t\t\tresult[p.name] = -1\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def try_parse_json(path: Path, sample_limit: int = 50) -> Tuple[str, Optional[int], List[Dict[str, Any]]]:\n",
    "\t\"\"\"\n",
    "\tAttempt to parse file as JSON array/object; fallback to JSONL sampling.\n",
    "\tReturns: (type_str, length_or_None, samples)\n",
    "\t\"\"\"\n",
    "\t# Try standard JSON\n",
    "\ttry:\n",
    "\t\twith open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tif isinstance(data, list):\n",
    "\t\t\treturn (\"array\", len(data), data[: sample_limit])\n",
    "\t\tif isinstance(data, dict):\n",
    "\t\t\treturn (\"object\", None, [data])\n",
    "\t\treturn (type(data).__name__, None, [])\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\n",
    "\t# Try JSON Lines\n",
    "\tsamples: List[Dict[str, Any]] = []\n",
    "\ttry:\n",
    "\t\twith open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "\t\t\tfor idx, line in enumerate(f):\n",
    "\t\t\t\tif not line.strip():\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tobj = json.loads(line)\n",
    "\t\t\t\t\tif isinstance(obj, dict):\n",
    "\t\t\t\t\t\tsamples.append(obj)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsamples.append({\"value\": obj})\n",
    "\t\t\t\texcept Exception:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif len(samples) >= sample_limit:\n",
    "\t\t\t\t\tbreak\n",
    "\texcept Exception:\n",
    "\t\treturn (\"unknown\", None, [])\n",
    "\treturn (\"jsonl\", None, samples)\n",
    "\n",
    "\n",
    "def infer_keys(samples: List[Dict[str, Any]], max_keys: int = 200) -> List[str]:\n",
    "\tseen = []\n",
    "\tseen_set = set()\n",
    "\tfor s in samples:\n",
    "\t\tif isinstance(s, dict):\n",
    "\t\t\tfor k in s.keys():\n",
    "\t\t\t\tif k not in seen_set:\n",
    "\t\t\t\t\tseen.append(k)\n",
    "\t\t\t\t\tseen_set.add(k)\n",
    "\t\tif len(seen) >= max_keys:\n",
    "\t\t\tbreak\n",
    "\treturn seen\n",
    "\n",
    "print(\"Setup OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List JSON files and gather basic stats (size, lines)\n",
    "from datetime import datetime\n",
    "\n",
    "files = sorted([p for p in DATA_DIR.glob(\"*_entities_dataset_v2.json\") if p.is_file()])\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "sizes_mb = {p.name: bytes_to_mb(p.stat().st_size) for p in files}\n",
    "line_counts = count_lines_fast(files[:200])  # count for first 200 to keep it quick; adjust as needed\n",
    "\n",
    "rows = []\n",
    "for p in files:\n",
    "\trows.append({\n",
    "\t\t\"filename\": p.name,\n",
    "\t\t\"size_mb\": sizes_mb.get(p.name, None),\n",
    "\t\t\"lines\": line_counts.get(p.name, None),\n",
    "\t\t\"modified\": datetime.fromtimestamp(p.stat().st_mtime).isoformat(timespec=\"seconds\"),\n",
    "\t})\n",
    "\n",
    "if pd is not None:\n",
    "\tdf_files = pd.DataFrame(rows)\n",
    "\tprint(df_files.head(10))\n",
    "\tout_csv = OUTPUT_DIR / \"entities_dataset_v2_file_stats.csv\"\n",
    "\tdf_files.to_csv(out_csv, index=False)\n",
    "\tprint(\"Saved:\", out_csv)\n",
    "else:\n",
    "\tprint(rows[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into a few files to infer structure and show samples\n",
    "from itertools import islice\n",
    "\n",
    "sample_files = files[:5] if len(files) > 0 else []\n",
    "summary = []\n",
    "\n",
    "for p in sample_files:\n",
    "\tkind, length, samples = try_parse_json(p, sample_limit=5)\n",
    "\tkeys_union = infer_keys(samples, max_keys=200)\n",
    "\tsummary.append({\n",
    "\t\t\"filename\": p.name,\n",
    "\t\t\"kind\": kind,\n",
    "\t\t\"length\": length,\n",
    "\t\t\"num_samples\": len(samples),\n",
    "\t\t\"keys\": keys_union,\n",
    "\t})\n",
    "\tprint(\"====\", p.name)\n",
    "\tprint(\"type:\", kind, \"length:\", length, \"keys (up to 20):\", keys_union[:20])\n",
    "\tfor i, s in enumerate(samples[:3]):\n",
    "\t\tprint(f\"sample[{i}] =\", s)\n",
    "\n",
    "if pd is not None and summary:\n",
    "\tdf_summary = pd.DataFrame(summary)\n",
    "\tdisplay(df_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate keys across many shards (union) to see overall schema\n",
    "from collections import Counter\n",
    "\n",
    "max_files = min(50, len(files))  # adjust for speed\n",
    "key_counter = Counter()\n",
    "\n",
    "for p in files[:max_files]:\n",
    "\tkind, length, samples = try_parse_json(p, sample_limit=30)\n",
    "\tfor s in samples:\n",
    "\t\tif isinstance(s, dict):\n",
    "\t\t\tfor k in s.keys():\n",
    "\t\t\t\tkey_counter[k] += 1\n",
    "\n",
    "key_stats = sorted(key_counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "print(\"Top 30 keys:\")\n",
    "for k, c in key_stats[:30]:\n",
    "\tprint(f\"{k}: {c}\")\n",
    "\n",
    "if pd is not None and key_stats:\n",
    "\tdf_keys = pd.DataFrame(key_stats, columns=[\"key\", \"count_in_samples\"])\n",
    "\tout_csv = OUTPUT_DIR / \"entities_dataset_v2_key_counts.csv\"\n",
    "\tdf_keys.to_csv(out_csv, index=False)\n",
    "\tprint(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Build a tiny sample dataframe for downstream EDA\n",
    "# Tries to normalize common fields if present\n",
    "\n",
    "common_rows = []\n",
    "max_items = 2000\n",
    "col_guess = set()\n",
    "\n",
    "for p in files[:10]:\n",
    "\tkind, length, samples = try_parse_json(p, sample_limit=300)\n",
    "\tfor obj in samples:\n",
    "\t\tif not isinstance(obj, dict):\n",
    "\t\t\tcontinue\n",
    "\t\tcol_guess.update(obj.keys())\n",
    "\t\tcommon_rows.append(obj)\n",
    "\t\tif len(common_rows) >= max_items:\n",
    "\t\t\tbreak\n",
    "\tif len(common_rows) >= max_items:\n",
    "\t\tbreak\n",
    "\n",
    "if pd is not None and common_rows:\n",
    "\tdf_sample = pd.DataFrame(common_rows)\n",
    "\tprint(df_sample.shape)\n",
    "\tdisplay(df_sample.head(5))\n",
    "\tout_csv = OUTPUT_DIR / \"entities_dataset_v2_sample_rows.csv\"\n",
    "\tdf_sample.to_csv(out_csv, index=False)\n",
    "\tprint(\"Saved:\", out_csv)\n",
    "else:\n",
    "\tprint(\"No rows or pandas not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
